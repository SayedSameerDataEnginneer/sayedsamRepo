\documentclass[10pt, letterpaper]{article}
\usepackage{ragged2e}
% Packages:
\usepackage[
    ignoreheadfoot, % set margins without considering header and footer
    top=0.5 cm, % seperation between body and page edge from the top
    bottom=0.5 cm, % seperation between body and page edge from the bottom
    left=1.5 cm, % seperation between body and page edge from the left
    right=1.5 cm, % seperation between body and page edge from the right
    footskip=1.0 cm, % seperation between body and footer
    % showframe % for debugging 
]{geometry} % for adjusting page geometry
\usepackage{titlesec} % for customizing section titles
\usepackage{tabularx} % for making tables with fixed width columns
\usepackage{array} % tabularx requires this
\usepackage[dvipsnames]{xcolor} % for coloring text
\definecolor{primaryColor}{RGB}{0, 0, 0} % define primary color
\usepackage{enumitem} % for customizing lists
\usepackage{fontawesome5} % for using icons
\usepackage{amsmath} % for math
\usepackage[
    pdftitle={John Doe's CV},
    pdfauthor={John Doe},
    pdfcreator={LaTeX with RenderCV},
    colorlinks=true,
    urlcolor=primaryColor
]{hyperref} % for links, metadata and bookmarks
\usepackage[pscoord]{eso-pic} % for floating text on the page
\usepackage{calc} % for calculating lengths
\usepackage{bookmark} % for bookmarks
\usepackage{lastpage} % for getting the total number of pages
\usepackage{changepage} % for one column entries (adjustwidth environment)
\usepackage{paracol} % for two and three column entries
\usepackage{ifthen} % for conditional statements
\usepackage{needspace} % for avoiding page brake right after the section title
\usepackage{iftex} % check if engine is pdflatex, xetex or luatex

% Ensure that generate pdf is machine readable/ATS parsable:
\ifPDFTeX
    \input{glyphtounicode}
    \pdfgentounicode=1
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{inputenc}
    \usepackage{lmodern}
\fi

\usepackage{charter}

% Some settings:
\raggedright
\AtBeginEnvironment{adjustwidth}{\partopsep0pt} % remove space before adjustwidth environment
\pagestyle{empty} % no header or footer
\setcounter{secnumdepth}{0} % no section numbering
\setlength{\parindent}{0pt} % no indentation
\setlength{\topskip}{0pt} % no top skip
\setlength{\columnsep}{0.15cm} % set column seperation
\pagenumbering{gobble} % no page numbering

\titleformat{\section}{\needspace{4\baselineskip}\bfseries\large}{}{0pt}{}[\vspace{1pt}\titlerule]

\titlespacing{\section}{
    % left space:
    -1pt
}{
    % top space:
    0.3 cm
}{
    % bottom space:
    0.2 cm
} % section title spacing

\renewcommand\labelitemi{$\vcenter{\hbox{\small$\bullet$}}$} % custom bullet points
\newenvironment{highlights}{
    \begin{itemize}[
        topsep=0.10 cm,
        parsep=0.10 cm,
        partopsep=0pt,
        itemsep=0pt,
        leftmargin=0 cm + 10pt
    ]
}{
    \end{itemize}
} % new environment for highlights


\newenvironment{highlightsforbulletentries}{
    \begin{itemize}[
        topsep=0.10 cm,
        parsep=0.10 cm,
        partopsep=0pt,
        itemsep=0pt,
        leftmargin=10pt
    ]
}{
    \end{itemize}
} % new environment for highlights for bullet entries

\newenvironment{onecolentry}{
    \begin{adjustwidth}{
        0 cm + 0.00001 cm
    }{
        0 cm + 0.00001 cm
    }
}{
    \end{adjustwidth}
} % new environment for one column entries

\newenvironment{twocolentry}[2][]{
    \onecolentry
    \def\secondColumn{#2}
    \setcolumnwidth{\fill, 4.5 cm}
    \begin{paracol}{2}
}{
    \switchcolumn \raggedleft \secondColumn
    \end{paracol}
    \endonecolentry
} % new environment for two column entries

\newenvironment{threecolentry}[3][]{
    \onecolentry
    \def\thirdColumn{#3}
    \setcolumnwidth{, \fill, 4.5 cm}
    \begin{paracol}{3}
    {\raggedright #2} \switchcolumn
}{
    \switchcolumn \raggedleft \thirdColumn
    \end{paracol}
    \endonecolentry
} % new environment for three column entries

\newenvironment{header}{
    \setlength{\topsep}{0pt}\par\kern\topsep\centering\linespread{1.5}
}{
    \par\kern\topsep
} % new environment for the header

\newcommand{\placelastupdatedtext}{% \placetextbox{<horizontal pos>}{<vertical pos>}{<stuff>}
  \AddToShipoutPictureFG*{% Add <stuff> to current page foreground
    \put(
        \LenToUnit{\paperwidth-2 cm-0 cm+0.05cm},
        \LenToUnit{\paperheight-1.0 cm}
    ){\vtop{{\null}\makebox[0pt][c]{
        \small\color{gray}\textit{Last updated in September 2024}\hspace{\widthof{Last updated in September 2024}}
    }}}%
  }%
}%

% save the original href command in a new command:
\let\hrefWithoutArrow\href

% new command for external links:


\begin{document}
    \newcommand{\AND}{\unskip
        \cleaders\copy\ANDbox\hskip\wd\ANDbox
        \ignorespaces
    }
    \newsavebox\ANDbox
    \sbox\ANDbox{$|$}

    \begin{header}
        \fontsize{25 pt}{25 pt}\selectfont Sayed Sameer

        \vspace{3 pt}

        \normalsize
        \mbox{Hyderabad, India}%
        \kern 5.0 pt%
        \AND%
        \kern 5.0 pt%
        \mbox{\hrefWithoutArrow{mailto:sayedsam2305@gmail.com}{sayedsam2305@gmail.com}}%
        \kern 5.0 pt%
        \AND%
        \kern 5.0 pt%
        \mbox{\hrefWithoutArrow{tel:9014504647}{9014504647}}%
        \kern 5.0 pt%
        \AND%
        \AND%
        \kern 5.0 pt%
        \mbox{\hrefWithoutArrow{https://www.linkedin.com/in/sayed-sameer-885939211}{linkedin.com/in/sayed-sameer}}%
        \kern 5.0 pt%
        % \AND%
        % \kern 5.0 pt%
        % \mbox{\hrefWithoutArrow{https://github.com/vikash-kum}{github.com/vikash-kum}}%
        % \kern 5.0 pt%
        % \AND%
        % \kern 5.0 pt%
        % \mbox{\hrefWithoutArrow{https://leetcode.com/vikashkum/}{leetcode.com/vikashkum}}%
    \end{header}

    \vspace{5 pt - 0.3 cm}


    \section{Summary}



        
        \begin{onecolentry}
        Certified Databricks Professional with 2.5+ years of experience in data engineering .\textbf{  Proficient in SQL, Python, PySpark, Azure Services,AWS Services and Azure Databricks. Seeking a challenging role to leverage my expertise in  optimizing data processes, analytics, and cloud solutions for impact-full business outcomes.}
           
        \end{onecolentry}

        \vspace{0.1 cm}

   

    
  

       \vspace{0.05 cm}
            
           

       
    
    \section{Experience}

        
        \begin{twocolentry}{
            August 2022 - Present
        }
           \underline{\textbf{Data Engineer}, LTIMindtree.} { 
              }\underline{Client :\textbf{Worlds Leading Insurance Brokerage, USA}}\\
          
            
            \end{twocolentry}

        \vspace{0.10 cm}
        \begin{onecolentry}
            \begin{highlights}
           \textbf {Project Overview: Designed and implemented an ETL pipeline utilizing AWS Glue and Databricks, seamlessly integrating insurance claim data across five countries into a centralized Tableau dashboard for enhanced business intelligence. .} \\
           \item

\item Engineered Databricks notebooks for seamless integration with Azure Event Hub, streamlining real-time data ingestion and optimizing storage in Azure Data Lake Storage (ADLS), supporting enhanced decision-making processes across five countries.



    \item Hands-on Experience on Glue to generic Spark applications using PySpark and Spark SQL for efficient data extraction, transformation and aggregation from diverse file formats.

 
    \item Utilized \textbf{AWS services including S3, Glue and Spark scripts} for data processing and analysis.
            \item Developed and implemented PySpark scripts that transformed over 500,000 records of JSON data into a flattened format; streamlined data processing workflows by reducing transformation time from hours to just minutes .    

             \item Gained substantial hands-on experience with Glue in conjunction with generic Spark applications.
             \item Engineered robust ETL processes using AWS Glue with PySpark and Spark SQL, extracting and transforming 600,000 records from diverse file formats monthly for accurate data aggregation to support business intelligence initiatives.
             
             \item  Collaborated on ETL (Extract, Transform, Load) tasks, ensuring data integrity and pipeline stability. Partnered with cross-functional teams to guarantee seamless data flow and system consistency, enhancing overall data reliability. 


              \item  Devised robust ETL pipelines utilizing AWS Glue alongside Spark SQL; optimized processes reduced average latency in report generation by 40 hours monthly while ensuring thorough aggregation of diverse file formats essential for business intelligence initiatives 

             \item Pioneered the creation of custom ETL procedures focused on increasing data quality assurance measures; findings directly contributed to addressing three critical issues within existing pipelines impacting overall performance stability.


             \item Designed scalable ETL pipelines for improved data ingestion, processing, and storage. Leveraged cloud-based solutions to handle large volumes of data and ensure scalability and performance.
            \end{highlights}
        \end{onecolentry}

          
     \section{Technical Skills}

        
       
        \begin{onecolentry}
           \underline{\textbf{ETL Tools:}} \textbf{Databricks, AWS Glue}, Azure Data Factory.
            
           

        \end{onecolentry}

       

        \vspace{0.1 cm}
 \begin{onecolentry}
            \underline{\textbf{Languages:}} \textbf{SQL, Python, PySpark}.\hspace{1cm} 
             \underline{\textbf{Database:}} \textbf{Hive, Oracle}, Db2.
             \hspace{0.7cm}
             \underline{\textbf{Cloud Platform:}} \textbf{AWS, Azure}. 
             
          
           
        \end{onecolentry}

         \vspace{0.1 cm}

        \begin{onecolentry}
        
         \underline{\textbf{Technologies:}}  Github, VScode, Jupiter Nb, Anaconda, UNIX, Jira, ADO( Agile), Power BI.
         
        \end{onecolentry}


         

        

        \vspace{0.1 cm}


    

\section{Certifications}
 \vspace{0.10 cm}
        \begin{onecolentry}
            \begin{highlights}
    
    \textbf{• Databricks Certified Data Engineer Professional}
     \href{https://credentials.databricks.com/7f8c8647-c2cc-41e0-b1bd-c7850156c4cc#acc.EVQW3dwg}{\color{blue} Certification Link}\hfill \textbf{• AWS Cloud Practitioner}   \href{https://cp.certmetrics.com/amazon/en/public/verify/credential/595c1ec9bfb748d99baf9e64095d1ea6}{\color{blue} Certification Link}\\
    
    
      
     \textbf{• Databricks Certified Data Engineer Associate}
    \href{https://credentials.databricks.com/a869d049-3308-46a7-b4cf-ffa209ec96e0#acc.bt3Siuyg}{\color{blue} Certification Link} \hfill
     \textbf{• Azure DP -203\hspace{2.45cm}} { {Currently Up-Skilling}}
     
     
     
\end{highlights}
        \end{onecolentry}
        





 

 
 


 \section{Education}

        
        \begin{twocolentry}{
            Aug 2018 – July 2022
        }
            \textbf{B.Tech  (CGPA: 8.9/10.0), CBIT Kadapa Andhra Pradesh.} \end{twocolentry}



%-----------project section-------------

   \section{Project}

        
        \begin{twocolentry}{
           Azure Databricks \& Azure Data Factory. 
        }
           {\textbf{-- Healthcare Revenue Cycle Management (RCM)  :}}\end{twocolentry}

        \vspace{0.10 cm}
        \begin{onecolentry}
            \begin{highlights}
           \item RCM encompasses the financial processes hospitals use, starting from patient appointment scheduling to the final payment received by the provider. 
            \end{highlights}
        \end{onecolentry}

% research intern

    
        \begin{twocolentry}{
            PySpark ,Azure Data Factory
        }
           {\textbf {-- COVID-19 Trend Analysis using PySpark and Azure Data Factory  :}}\end{twocolentry}

        \vspace{0.10 cm}
        \begin{onecolentry}
            \begin{highlights}
            
            \item Utilized big data tools to perform trend analysis, enabling timely responses and improving hospital treatment plans during the COVID-19 pandemic.

            \end{highlights}
        \end{onecolentry}




 

\end{document}
