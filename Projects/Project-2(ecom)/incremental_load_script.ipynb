{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4376195-a353-471a-8415-91746d464135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585c8b09-9ca2-4810-b799-03b9c9dd834f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0279687-91ef-400f-8ffc-7ccfb632316f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Function returning config.tbl_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcab34c2-f2ce-4b5d-a6e7-c345498286f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def config_df():\n",
    "    \"\"\"\n",
    "    This function rteurns the workspace.config.tbl_details Dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.sql('select * from workspace.config.tbl_details')\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2539fa-30e1-4a20-89a4-b6cffe066b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Looping through the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fcb132-c6ff-42c2-be88-5215a0d7dbef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_src_tgt_names(df):\n",
    "    \"\"\"\n",
    "    This function loops through the dataframe which we get from the config_df() function \n",
    "    and \n",
    "    returns a list of tuples of the form (src_tbl_t_name,tgt_tbl_t_name,incremental_column_name,tgt_inc_column)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_details=[]\n",
    "        for x in df.collect():\n",
    "            src_catalog = x['src_catalog_name']\n",
    "            src_database=x['src_database_name']\n",
    "            src_tbl=x['src_table_name']\n",
    "            inc_col=x['incremental_column_name']\n",
    "            src_tbl_t_name=src_catalog+'.'+src_database+'.'+src_tbl\n",
    "            tgt_catalog=x['tgt_catalog_name']\n",
    "            tgt_schema=x['tgt_schema_name']\n",
    "            tgt_tbl=x['tgt_tbl_name']\n",
    "            tgt_inc_col=x['tgt_inc_column']\n",
    "            tgt_tbl_t_name=tgt_catalog+'.'+tgt_schema+'.'+tgt_tbl\n",
    "            table_details.append((src_tbl_t_name,tgt_tbl_t_name,inc_col,tgt_inc_col))\n",
    "        return table_details\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a811123-9405-4723-985f-dea90b4d679a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Getting last Refresh data run from config.control_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd54958-2f5e-409f-9aa3-621f6d33ba9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_refresh_date(up_col_name, src_file_name):\n",
    "    \"\"\"\n",
    "    This function gets the last refresh date from the control table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "            SELECT NVL(MAX({up_col_name}), TO_DATE('1900-01-01')) AS update_date\n",
    "            FROM workspace.config.control_tbl\n",
    "            WHERE tbl_name = '{src_file_name}'\n",
    "            \n",
    "        \"\"\"\n",
    "        older_date = spark.sql(query)\n",
    "        fallback_date = older_date.collect()[0]['update_date'].strftime('%d-%m-%Y')\n",
    "        return fallback_date\n",
    "    except Exception as e:\n",
    "        print(f\"Failed due to Exception: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10360f31-e816-467a-8452-b4496d5aabc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Creating of source table Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedac9dc-ed99-49e9-b76f-3f99db451d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def creating_src_df(src_tbl_t_name,incremental_column_name,fallback):\n",
    "    \"\"\"\n",
    "    This function creates a dataframe from the data which is available in the source table \n",
    "    and returns the dataframe and count of records\n",
    "    \"\"\"\n",
    "    try:\n",
    "    \n",
    "        df=spark.sql(f\"\"\"\n",
    "                    select * \n",
    "                    from {src_tbl_t_name}\n",
    "                    where \n",
    "                    to_date ({incremental_column_name},\"dd-MM-yyyy\" )> TO_DATE('{fallback}','dd-MM-yyyy')\n",
    "                    \"\"\").withColumn(\"ingestion_timestamp\",current_date())\n",
    "        row_count=df.count()\n",
    "        return df,row_count\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e82879-3d8b-49ab-95cb-d07f89f4b01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Writing the data to Traget tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc4ac09-6d7d-4e84-bb93-b90451ddb9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_data_to_bronze(df,tgt_tbl_t_name,row_count):\n",
    "    \"\"\"\n",
    "    This function writes the data to the bronze table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write.format('delta').mode('append').saveAsTable(tgt_tbl_t_name)\n",
    "        if row_count>=0:\n",
    "            status=\"completed\"\n",
    "        else:\n",
    "            status=\"failed\"\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8463f23-06d4-4b99-95b7-ae493a61f044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Updating value in config.control_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416ea065-dee7-4c44-a479-ccd986cec9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_and_update_value(tgt_inc_column,tgt_tbl_t_name,status,src_tbl_t_name):\n",
    "    \"\"\"\n",
    "    This function updates the control table with the latest date and status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        New_date=spark.sql(f\"\"\"\n",
    "                    select max({tgt_inc_column}) as max_date\n",
    "                    from\n",
    "                    {tgt_tbl_t_name}\n",
    "\n",
    "                    \"\"\")\n",
    "        print(f\"New_date is {New_date}\")\n",
    "        if New_date.count()!=0:\n",
    "            Next_date=New_date.collect()[0]['max_date']\n",
    "        print(f\"Next data is {Next_date}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            Update workspace.config.control_tbl\n",
    "            set update_date='{Next_date}',\n",
    "            status='{status}'\n",
    "            where \n",
    "            tbl_name='{src_tbl_t_name}'\n",
    "            \"\"\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "            print(f\"failed due to Exception {e}\")\n",
    "            return None\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4083e96b-47e9-47ed-b3cc-b5de1f0c042f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750e1bbc-8258-4d10-a060-1c0aaf5facb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    df=config_df()\n",
    "    display(df)\n",
    "\n",
    "    for src_tbl_t_name,tgt_tbl_t_name,inc_col,tgt_col in get_src_tgt_names(df):\n",
    "        print(src_tbl_t_name)\n",
    "        print(tgt_tbl_t_name)\n",
    "        print(inc_col)\n",
    "\n",
    "\n",
    "        fall_back_date=get_last_refresh_date(up_col_name='update_date',src_file_name=src_tbl_t_name)\n",
    "        print(fall_back_date)\n",
    "        \n",
    "        df,count=creating_src_df(src_tbl_t_name=src_tbl_t_name,incremental_column_name=inc_col,fallback=fall_back_date)\n",
    "        print(df)\n",
    "        print(count)\n",
    "\n",
    "        status=write_data_to_bronze(df=df,tgt_tbl_t_name=tgt_tbl_t_name,row_count=count)\n",
    "        print(status)\n",
    "\n",
    "        result=get_and_update_value(tgt_inc_column=tgt_col,tgt_tbl_t_name=tgt_tbl_t_name,status=status,src_tbl_t_name=src_tbl_t_name)\n",
    "        print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be586db1-0f9c-4a86-8cfd-040f25f71c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.sql(f\"\"\"select nvl(max(update_date), to_date('01-01-1900','dd-MM-yyyy')) as update_date \n",
    "              from workspace.default.control_tbl \n",
    "              where tbl_name = 'workspace.default.order_details' \n",
    "              and status = 'completed'\"\"\"\n",
    "         )\n",
    "display(df.collect()[0]['update_date'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "incremental_load_script",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
