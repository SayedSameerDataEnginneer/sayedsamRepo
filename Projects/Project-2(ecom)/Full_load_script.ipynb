{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "104af1f4-a173-4be0-a56c-662efe456471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541ad4dd-e53f-4303-90f8-294ca7b26bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81a9ba1-151e-4942-9759-2271826e327d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Function returning config.tbl_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed338864-f81b-4afe-b59e-be7271d8282c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def config_df():\n",
    "    \"\"\"\n",
    "    This function rteurns the workspace.config.tbl_details Dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spark.sql('select * from workspace.config.tbl_details')\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a57c7515-212c-440b-bddc-3ee0ab25141e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Looping through the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d9fe2-e768-4bd7-9c3e-4238417a2c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_src_tgt_names(df):\n",
    "    \"\"\"\n",
    "    This function loops through the dataframe which we get from the config_df() function \n",
    "    and \n",
    "    returns a list of tuples of the form (src_tbl_t_name,tgt_tbl_t_name,incremental_column_name,tgt_inc_column)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_details=[]\n",
    "        for x in df.collect():\n",
    "            src_catalog = x['src_catalog_name']\n",
    "            src_database=x['src_database_name']\n",
    "            src_tbl=x['src_table_name']\n",
    "            inc_col=x['incremental_column_name']\n",
    "            src_tbl_t_name=src_catalog+'.'+src_database+'.'+src_tbl\n",
    "            tgt_catalog=x['tgt_catalog_name']\n",
    "            tgt_schema=x['tgt_schema_name']\n",
    "            tgt_tbl=x['tgt_tbl_name']\n",
    "            tgt_inc_col=x['tgt_inc_column']\n",
    "            tgt_tbl_t_name=tgt_catalog+'.'+tgt_schema+'.'+tgt_tbl\n",
    "            table_details.append((src_tbl_t_name,tgt_tbl_t_name,inc_col,tgt_inc_col))\n",
    "        return table_details\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31bf4089-74c7-4ab1-bc1e-c025823345a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Truncating tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7afc67-22d6-43c5-9d5b-a7ee920384f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def truncate_tbl(tbl_name):\n",
    "    \"\"\"\n",
    "    This functions helps us to truncate the bronze tables as we are going for a Full load\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"truncate table {tbl_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51dcfe63-be3d-41e1-8ff5-e13ca1eacfd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Creating of source table Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcfdc64-68b1-443f-81c7-7183ebbc21ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def creating_src_df(src_tbl_t_name,incremental_column_name):\n",
    "    \"\"\"\n",
    "    This function creates a dataframe from the data which is available in the source table \n",
    "    and returns the dataframe and count of records\n",
    "    \"\"\"\n",
    "    try:\n",
    "    \n",
    "        df=spark.sql(f\"\"\"\n",
    "                    select * \n",
    "                    from {src_tbl_t_name}\n",
    "                    where \n",
    "                    to_date ({incremental_column_name},\"dd-MM-yyyy\" )> TO_DATE('01-01-1900','dd-MM-yyyy')\n",
    "                    \"\"\").withColumn(\"ingestion_timestamp\",current_date())\n",
    "        row_count=df.count()\n",
    "        return df,row_count\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f872d34-a121-4628-ad48-47220dfa3850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Writing the data to Traget tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6a6f87-2d5c-4797-9fe8-aa536b3e5205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_data_to_bronze(df,tgt_tbl_t_name,row_count):\n",
    "    \"\"\"\n",
    "    This function writes the data to the bronze table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write.format('delta').mode('append').saveAsTable(tgt_tbl_t_name)\n",
    "        if row_count>=0:\n",
    "            status=\"completed\"\n",
    "        else:\n",
    "            status=\"failed\"\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to Exception {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "373d7eda-9470-46b2-8c41-ff3b6c10da6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Updating value in config.control_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc331375-af21-49c0-9159-2c7582f2563b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_and_update_value(tgt_inc_column,tgt_tbl_t_name,status,src_tbl_t_name):\n",
    "    \"\"\"\n",
    "    This function updates the control table with the latest date and status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        New_date=spark.sql(f\"\"\"\n",
    "                    select max({tgt_inc_column}) as max_date\n",
    "                    from\n",
    "                    {tgt_tbl_t_name}\n",
    "\n",
    "                    \"\"\")\n",
    "        print(f\"New_date is {New_date}\")\n",
    "        if New_date.count()!=0:\n",
    "            Next_date=New_date.collect()[0]['max_date']\n",
    "        print(f\"Next data is {Next_date}\")\n",
    "        spark.sql(f\"\"\"\n",
    "            Update workspace.config.control_tbl\n",
    "            set update_date='{Next_date}',\n",
    "            status='{status}'\n",
    "            where \n",
    "            tbl_name='{src_tbl_t_name}'\n",
    "            \"\"\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "            print(f\"failed due to Exception {e}\")\n",
    "            return None\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d280fe97-a473-41bb-b475-dd6977a3b279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb109b7-e0cd-42cd-98fa-ce3de420d509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    df=config_df()\n",
    "    display(df)\n",
    "\n",
    "    for src_tbl_t_name,tgt_tbl_t_name,inc_col,tgt_col in get_src_tgt_names(df):\n",
    "        print(src_tbl_t_name)\n",
    "        print(tgt_tbl_t_name)\n",
    "        print(inc_col)\n",
    "\n",
    "        ans =truncate_tbl(tgt_tbl_t_name)\n",
    "        print(f\"deleted from table {tgt_tbl_t_name} sucessfully,{ans}\")\n",
    "\n",
    "        df,count=creating_src_df(src_tbl_t_name=src_tbl_t_name,incremental_column_name=inc_col)\n",
    "        print(df)\n",
    "        print(count)\n",
    "\n",
    "        status=write_data_to_bronze(df=df,tgt_tbl_t_name=tgt_tbl_t_name,row_count=count)\n",
    "        print(status)\n",
    "\n",
    "        result=get_and_update_value(tgt_inc_column=tgt_col,tgt_tbl_t_name=tgt_tbl_t_name,status=status,src_tbl_t_name=src_tbl_t_name)\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8002633148784031,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Full_load_script",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
